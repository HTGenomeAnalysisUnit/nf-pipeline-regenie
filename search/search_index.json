{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"nf-pipeline-regenie","text":"<p>A nextflow pipeline to perform genome-wide association studies (GWAS) and rare variant association analysis using regenie at high speed.</p> <p></p>"},{"location":"#main-features","title":"Main features","text":"<ul> <li>The pipeline allows great flexibility in how operation are parallelized to allow balacing resource usage and speed and to accomodate different dataset sizes. The default settings are optimized for biobank scale data, but you can adjust resources as needed to further cut down run time or to process smaller datasets (see the optimize parallelization section).</li> <li>All major data types are accepted as input, including plink1 binary dataset (bed/bim/fam), plink2 binary dataset (pgen/pvar/psam), bgen format (bgen/bgi/sample), and vcf.gz format.</li> <li>The pipeline can perform both standard GWAS analysis on single variants, and aggregated rare variant tests using burden test and any of the tests available in regenie, namely skat, skato, sakto-acat, acatv, acato, acato-full.</li> <li>Results include summary statistics per phenotype, and also filtered tophits / loci annotated with nearby genes and an HTML report for each phenotype with Manhattan plot and regional plots for the best loci.</li> <li>You can run a single analysis project that can combine any number of phenotypes and covariates in a single analysis as long as you use a single model and phenotypes are all quantitative or all binary.</li> <li>Alternatively you can configure multiple analysis at once using multi models mode or multi projects mode. Using the multi models / projects mode it is possible to fully automate the test of multiple association models for a cohort. In multi-model mode the pipeline will take care of setting up uniform analysis groups given the provided models, while multi-project mode allows to run multiple independent analysis in parallel given a general configuration table.</li> </ul>"},{"location":"#how-to-use","title":"How to use","text":"<p>The suggested way to run the pipeline is to create a config file defining your computations environment (see the hpc profile section) and a config file for your project (see the main parameters section). You can use the templates provided in the <code>templates</code> folder.</p> <p>Then you can invoke the pipeline using <code>nextflow run HTGenomeAnalysisUnit/nf-pipeline-regenie -profile singularity,myprofile -c your_project.conf -c your_profile.conf</code></p> <p>See the quick start section for a minimal example.</p>"},{"location":"#credits","title":"Credits","text":"<p>The original concept is based on this amazing github repository from Institute of Genetic Epidemiology, Innsbruck maintained by Sebastian Sch\u00f6nherr and Lukas Forer.</p>"},{"location":"additional-notes/","title":"Additional notes","text":""},{"location":"additional-notes/#mac-filtering","title":"MAC filtering","text":"<p>Note that the pipeline impose a MAC (100) and a MAF (0.01) filter on genotyped data used for step1, as reccomended by regenie UKBB tutorial, and a min MAC 50 at step2 as suggested in the recent large proteome analysis of UKBB (see the preprint). </p> <p>The min MAC for variants accepted in step2 can be adjusted by setting the <code>regenie_rarevar_min_mac</code> and <code>regenie_gwas_min_mac</code> parameters in your profile.</p> <p>This MAC filters are applied after subsetting the data to contain only samples seen in the phenotype table. FID/IID are extracted from the pheno file and used with <code>--keep</code> in genotyped data QC to generate a QCed dataset used in regenie step 1. Then in step 2, regenie automatically removes any sample not seen in step 1 predictions before applying the MAC filter.</p> <p>As a result of this process, if you use only a subset of samples present in the original imputed data, it is possible that some variants will be filtered out due to low MAC. Check log files and the results table to see exactly how many variants were considered in the analysis.</p>"},{"location":"additional-notes/#clumping","title":"Clumping","text":"<p>When clumping is active, the pipeline will save clumped data and resulting loci with genes annotation in the <code>toploci</code> folder. Note that if there are multiple identical SNP IDs clumping will fail. To avoid this you can for example modify your SNP ids to include ref/alt alleles as follows: <code>[SNPID]_[A0]_[A1]</code>.</p> <p>If you are using LD panel files as described in the LD panel section, please ensure that SNP ids are concordant between bed/bim/fam files in the LD_panel and the input  dataset for GWAS analysis, otherwise SNPs that are not found will be dropped from clumping report.</p>"},{"location":"full-params-list/","title":"Full list of parameters","text":""},{"location":"full-params-list/#global-settings-of-resources","title":"Global settings of resources","text":"<p>Global settings for max resources available per process in your system</p> <ul> <li>max_memory </li> <li>max_cpus   </li> <li>max_time   </li> </ul>"},{"location":"full-params-list/#general-settings-per-run","title":"General settings per run","text":"<ul> <li>project: project name. Max 50 chars. No spaces allowed. Use only alphanumeric, underscores, or dashes</li> <li>project_date: usually configured automatically to execution date, but you can override the date manually</li> <li>outdir: a specific outdir for the project, if null the project id is used as output dir, otherwise <code>outdir/project_id/</code></li> <li>chromosomes: chromosomes to include in the analysis. Accepts comma separated lists and intervals like <code>1-5,8-12,22</code></li> <li>master_log_dir: set this to redirect all logs to a specific folder</li> </ul>"},{"location":"full-params-list/#multi-model-specific-settings","title":"Multi-model specific settings","text":"<pre><code>models_table                          = null //when a table is provided, this activates master table mode\npheno_chunk_size                      = 50 //maximum number of phenotypes for a single GWAS run\nmissing_tolerance                     = 0.1 //max fraction of missing values per phenotype when assembling execution groups\n</code></pre>"},{"location":"full-params-list/#input-files","title":"Input files","text":""},{"location":"full-params-list/#genetic-data","title":"Genetic data","text":"<pre><code>genotypes_array                       = null //genotypes data for step 1. Can be bgen, pgen, bed or vcf.gz\ngenotypes_imputed                     = null //genotype data for gwas analysis. Can be bgen, pgen, bed or vcf.gz\ngenotypes_rarevar                     = null //genotype data for rare variants analysis. Can be bgen, pgen, plink bed or vcf.gz\ngenotypes_build                       = null //genome build\ngenotypes_imputed_format              = null //input data format for gwas. Can be bgen, pgen, bed or vcf\ngenotypes_rarevar_format              = null //input data format for rare variants. Can be bgen, pgen, bed or vcf\nimputed_sample_file                   = 'NO_SAMPLE_FILE' //Provide a specific sample file to use with gwas bgen input\nrarevar_sample_file                   = 'NO_SAMPLE_FILE' //Provide a specific sample file to use with rare variants bgen input\nld_panel                              = 'NO_LD_FILE' //A pattern pointing to a subset of genotypes to be used for LD computation. Optional, but highly reccomended for large datasets\n</code></pre>"},{"location":"full-params-list/#phenotypes","title":"Phenotypes","text":"<pre><code>phenotypes_filename                   = null //table of phenos - required\nphenotypes_columns                    = null //comma separated list of col names in pheno file to be analyzed\nphenotypes_binary_trait               = null //true for binary\n</code></pre>"},{"location":"full-params-list/#covariates","title":"Covariates","text":"<pre><code>covariates_filename                   = 'NO_COV_FILE' //files containing covariates, use NO_COV_FILE when absent\ncovariates_columns                    = '' //comma-separated list of covariates column names \ncovariates_cat_columns                = '' //comma-separated list of categorical covariate column names\nmaxCatLevels                          = 10 //maximum number of allowed levels for categorical covars\n</code></pre>"},{"location":"full-params-list/#rare-variants-accessory-files","title":"Rare variants accessory files","text":"<p>These are mandatory when running rare var analysis</p> <pre><code>rarevar_set_list_file                 = null //set list file as defined in regenie docs\nrarevar_anno_file                     = null //variant annotation file as defined in regenie docs\nrarevar_mask_file                     = null //mask definition file as defined in regenie docs\n</code></pre>"},{"location":"full-params-list/#other-optional-inputs","title":"Other optional inputs","text":"<pre><code>genes_bed                             = false //an optional .bed file specifying genes intervals used for results annotation\ngenes_ranges                          = false //an optional .interval file specifying genes intervals used for loci annotation\n</code></pre>"},{"location":"full-params-list/#step1-pre-processing-settings","title":"STEP1 PRE-PROCESSING SETTINGS","text":""},{"location":"full-params-list/#snp-pruning","title":"SNP pruning","text":"<pre><code>prune_enabled                         = false\nprune_maf                             = 0.01\nprune_window_kbsize                   = 1000\nprune_step_size                       = 100\nprune_r2_threshold                    = 0.9\n</code></pre>"},{"location":"full-params-list/#filtering","title":"Filtering","text":"<pre><code>qc_maf                                = '0.01'\nqc_mac                                = '100'\nqc_geno                               = '0.05'\nqc_hwe                                = '1e-15'\nqc_mind                               = '0.1'\n</code></pre>"},{"location":"full-params-list/#regenie-step1-settings","title":"REGENIE STEP1 SETTINGS","text":"<pre><code>regenie_bsize_step1                   = 1000\nregenie_premade_predictions           = false //or pattern to regenie step1 files\nsave_step1_predictions                = true\nregenie_force_step1                   = false\nregenie_ref_first_step1               = false\nstep1_use_loocv                       = false\nstep1_niter                           = 30\nstep1_n_chunks                        = 100 // N chunks when performing step1 L0 regression\n</code></pre>"},{"location":"full-params-list/#regenie-step2-settings","title":"REGENIE STEP2 SETTINGS","text":""},{"location":"full-params-list/#general-settings","title":"General settings","text":"<pre><code>phenotypes_delete_missings            = false //remove samples with missing data at any of the phenotypes\nregenie_bsize_step2                   = 400\nregenie_ref_first_step2               = true\nregenie_skip_predictions              = false //skip reading the step1 predictions (corresponds to simple linear/logistic regression)\nregenie_range                         = '' // when splitting is not active you can use this to specify a genomic range for step2 analysis\nregenie_extract_snps                  = '' // when splitting is not active you can specify a file containing a list of variant IDs to restrict step2 analysis\nregenie_extract_genes                 = '' // when splitting is not active you can use this to specify a file containing a list of genes to restrict step2 analysis\ninteraction_cov                       = null // run GxE test in GWAS specifying the interacting covariate from covariate table\ninteraction_snp                       = null // run GxG test in GWAS specifying the interacting variant ID\ncondition_list                        = null // run conditional analysis in GWAS specifying a files with variant IDs to condition on\nadditional_geno_file                  = null // prefix of the genotype dataset containing vars in condition_list or interaction var. This is mandatory for conditional or interaction analysis\nadditional_geno_format                = null // can be bgen, pgen or bed.\nsave_chunks_file                      = true\nsave_step2_logs                       = true\nsave_bgen_index                       = true\nsave_bgen_sample                      = true    \nsave_converted_bgen                   = true\n</code></pre>"},{"location":"full-params-list/#vcf-to-pgen-conversion-settings","title":"VCF to PGEN conversion settings","text":"<pre><code>gwas_read_dosage_from                 = 'DS' //DS (usually for VCF from imputateion) or GP (usually VCF from sequencing)\nrarevar_read_dosage_from              = 'GP' //DS (usually for VCF from imputateion) or GP (usually VCF from sequencing)\nimport_dosage_certainty               = 0.7 //when using GP, the certainty threshold to import dosages. If none of the probabilities is above this, the genotype is set to missing\nvcf_fixed_fid                         = null //when null vcf is converted to pgen using --double-id, otherwise fid is fixed to this value\n</code></pre>"},{"location":"full-params-list/#gwas-analysis-settings","title":"GWAS analysis settings","text":"<pre><code>step2_gwas_split                      = true //when true activate split of step2 by variant chunks\nstep2_gwas_chunk_size                 = 100000 //n variants per chunk when running gwas in split mode\nregenie_gwas_test                     = 'additive' //additive, dominant, recessive\nregenie_min_imputation_score          = '0.00'\nregenie_gwas_min_mac                  = '50' // min MAC for variants to be included in step2 for gwas \nregenie_firth                         = true\nregenie_firth_approx                  = true\n</code></pre>"},{"location":"full-params-list/#rare-variant-analysis-settings","title":"Rare variant analysis settings","text":"<pre><code>step2_rarevar_split                   = true //when true activate split of step2 by gene chunks\nstep2_rarevar_chunk_size              = 200 //n genes per chunk when running rare variant test in split mode\nregenie_rarevar_min_mac               = '1' // min MAC for variants to be included in step2 for rare vars\nrarevars_aaf_bins                     = '0.01,0.05' //comma-separated list of AAF upper bounds to use when building masks for burden test\nrarevars_vc_test                      = 'skat,skato,acatv,acato' //comma-separated list of SKAT/ACAT-type tests to run\nrarevars_joint_test                   = 'minp,acat' //comma-separated list of joint tests to run. Choices: minp,acat,sbat. Note that sbat can't run on binary phenotypes\nrarevars_vc_maxAAF                    = '0.05' //AAF upper bound to use for SKAT/ACAT-type tests\nregenie_build_mask                    = 'max' //build mask for rare variant test. Can be max, sum, or a comphet\nrarevars_write_mask_snplist           = false //when true write list of variants that went into each mask to file\n</code></pre>"},{"location":"full-params-list/#gwas-results-annotation-and-clumping","title":"GWAS RESULTS ANNOTATION AND CLUMPING","text":"<pre><code>genes_group                           = 'protein_coding' //genes group to use for annotation. Can be all or protein_coding\nannotation_min_log10p                 = 7.3 //results with -log10(p) above this will be reported as top hits with annotated genes\nannotation_interval_kb                = 25\nclumping                              = true\nclump_p1                              = 5e-8\nclump_p2                              = 1e-4\nclump_kb                              = 250\n</code></pre>"},{"location":"full-params-list/#rare-variants-results-annotation","title":"RARE VARIANTS RESULTS ANNOTATION","text":"<pre><code>rarevar_min_log10p                    = 5 //results with -log10(p) above this will be reported as top hits\nrarevar_stat_test                     = \"BONF_bygroup\" //Stat value to filter on. Possible values: \"FDR_bygroup\", \"FDR_alltests\", \"BONF_bygroup\", \"BONF_alltests\"\nrarevar_stat_test_threshold           = 1.3 //results with -log10(stat_value) above this will be annotated in a dedicated manhattan plot\n</code></pre>"},{"location":"full-params-list/#report-settings","title":"REPORT SETTINGS","text":"<pre><code>make_report                           = true //it can be useful to disable when analyzing many phenotypes\nmanhattan_annotations                 = 'genes' //how to annotate peaks in manhattan plot. Either 'genes' or 'snpid'\nregional_plot_window_kb               = 300 //window size in kb for regional plots. This value is added on each side of the locus when plotting\nn_top_loci_plot                       = 5 //number of top loci to plot in regional plots\n</code></pre>"},{"location":"hpc-profile/","title":"Define HPC profile","text":"<p>To create a profile for your HPC cluster, copy the template file <code>templates/profile_template.conf</code> and adapt the parameters according to your HPC cluster configuration.</p> <p>You can use this to configure the scheduler, the queue names, the number of concurrent jobs, the memory and time limits, etc. for your specific environment. See the Nextflow documentation for more details.</p> <p>Then use the profile in the pipeline execution by providing the new config file and the profile name, something like <code>-c myprofile.conf -profile singularity,&lt;profile_name&gt;</code>.</p>"},{"location":"input-conditional-analysis/","title":"Conditional and interaction analysis","text":"<p>In case you want to perform conditional or interaction analysis, you can configure the following parameters:</p> <ul> <li> <p><code>interaction_cov</code>: run GxE test specifying the interacting covariate from covariate table.   For GxE tests where the interacting variable is categorical, you can specify the baseline level using <code>'VARNAME[BASE_LEVEL]'</code> (e.g. <code>'BMI[&lt;25]'</code>). Otherwise, the first value found in the covariate file will be used as the baseline level.</p> </li> <li> <p><code>interaction_snp</code>: run GxG test specifying the interacting variant ID   For GxG tests, the default coding for the interacting variant is additive. If you would like to use dominant/recessive/categorical coding, use <code>'SNP_NAME[dom/rec/cat]'</code> (for example with dominant coding, <code>'SNPNAME[dom]'</code> will allow for separate effects between carriers vs non-carriers of the interacting variant). The allowed values in the brackets are add/dom/rec/cat.</p> </li> <li> <p><code>condition_list</code>: run conditional analysis specifying a file with variant IDs to condition on</p> </li> </ul> <p>Note that to perform conditional/interaction analysis, an additional genotype dataset must be provided containing the SNP(s) used for conditioning/interaction. This can be configured using the following parameters:</p> <ul> <li><code>additional_geno_format</code>: can be bgen, pgen or bed.</li> <li><code>additional_geno_file</code>: prefix of the genotype dataset containing vars in condition_list or interaction var. This is mandatory for conditional or interaction analysis. Depending on the value of <code>additional_geno_format</code>, the pipeline expects specific files to be present</li> <li>bgen: <code>additional_geno_file.bgen</code>, <code>additional_geno_file.bgen.bgi</code>, <code>additional_geno_file.sample</code></li> <li>bed: <code>additional_geno_file.bed</code>, <code>additional_geno_file.bim</code>, <code>additional_geno_file.fam</code></li> <li>pgen: <code>additional_geno_file.pgen</code>, <code>additional_geno_file.psam</code>, <code>additional_geno_file.pvar</code></li> </ul>"},{"location":"input-full-data/","title":"Full genotype data","text":"<p>To perform genetic association or rare variants analysis (regenie step 2) you must provide the full genetic dataset for your study, usually a large genetic dataset from imputation or sequencing. No QC is performed on teh full genetic dataset so ensure it is clean.</p> <p>Input dataset for GWAS analysis is defined by <code>genotypes_imputed</code> and <code>genotypes_imputed_format</code> parameters, while input dataset for rare variants analysis is defined by <code>genotypes_rarevar</code> and <code>genotypes_rarevar_format</code>. The format parameter accepts 4 possible formats: vcf, bgen, pgen or bed.</p> <p>The input dataset can be provided as a single file, split in multiple chunk files, or split by chromosome including a <code>{CHROM}</code> tag in the input filename (see below).</p>"},{"location":"input-full-data/#input-formats","title":"Input formats","text":""},{"location":"input-full-data/#vcf-format","title":"vcf format","text":"<ul> <li>input format: <code>'vcf'</code></li> <li>input dataset: <code>'path/to/my_data.vcf.gz'</code></li> </ul> <p>The input VCF must be in .vcf.gz format and will be converted to <code>bgen</code> format using plink2 and <code>--const-fid 0</code> option. The following parameters are used to control the conversion (see relevant plink2 documentation):</p> <ul> <li><code>vcf_min_gq</code>: minimum genotype quality (GQ) for a genotype to be included in the PGEN output.</li> <li><code>gwas_read_dosage_from</code> / <code>rarevar_read_dosage_from</code>: these params can be set to decide from which field to read GT probabilities when converting VCF for the GWAS and rare variants input datasets, respectively. Accepted options are <code>'HDS'</code> (default) which usually works for VCF from Minimac4 imputation, <code>'DS'</code> for Minimac3 dosages or <code>'GP'</code> for genotype probabilities (to use with VCF from sequencing). Default is <code>null</code>, actual genotypes will be imported.</li> <li><code>import_dosage_certainty</code>: when <code>read_dosage_from = 'GP'</code> this parameter controls the minimum probability accepted to set a genotype. You can set to null to remove this filter. Suggetion is to use <code>0.7</code>.</li> <li><code>vcf_fixed_fid</code>: when <code>null</code> the conversion is performed using <code>--double-id</code> option, so that sample IDs in the VCF are used also for FID. If you set a string (like <code>'0'</code>) than the FID column in the VCF is fixed and set to this value for all samples. Default is <code>0</code>.</li> </ul> <p>The converted dataset is saved to output folder when <code>save_pgen = true</code> (default).</p> <p>NB1. Using the default setting the resulting pgen dataset will have constant FID set to '0' and IID corresponding to the sample IDs in the VCF. You need to be sure that this nomenclature is consistent with the covariates and phenotype input files.</p> <p>NB2. There are some aspects to keep in mind about the conversion from VCF to pgen format (see plink2 docs for more details):</p> <ul> <li>PGEN can store genotype dosage, but not genotype probabilities, thus probabilities are collapsed to dosages during conversion, which is usually fine for most analyses.</li> <li>When using <code>DS</code> or <code>HDS</code> during import, the dosage are read directly from the VCF.</li> <li>When using <code>GP</code>, the genotype probabilites are converted to dosages according to the <code>import_dosage_certainty</code> parameter. The default value of <code>0.7</code> means that a genotype dosage are set only if the probability of the most likely genotype is &gt;= 0.7, otherwise teh whole genotype is set to missing.</li> </ul>"},{"location":"input-full-data/#bgen-format","title":"bgen format","text":"<ul> <li>input format: <code>'bgen'</code></li> <li>input dataset: <code>'path/to/my_data.bgen'</code></li> </ul> <p>Some additional files are expected when using bgen format:</p> <ol> <li>A bgi index for you bgen file. For a dataset named <code>my_dataset.bgen</code> the expected name of index is <code>my_dataset.bgen.bgi</code>. You can generate this using bgenix tool. You can use something like <code>bgenix -g my_data.bgen -index</code>.</li> <li>A sample file. For a dataset named <code>my_dataset.bgen</code> the expected name of index is <code>my_dataset.sample</code>. This is standard sample file defined for the bgen format which contains sample level information.</li> </ol> <p>NB: When using BGEN input, make sure that the sample ID in the BGEN or sample file can match FID + IID present in the covariates and phenotype input files, otherwise the pipeline will fail. Using a <code>.sample</code> file can help to have better control on the sample IDs.</p> <p>If any of these files is missing, the pipeline will generate them automatically and save them in the output folder by default. This behaviour can be controlled by <code>save_bgen_index</code>, <code>save_bgen_sample</code> parameters. Keep in mind that these steps can add a significant amount of time to the overall execution for large datasets, so it is suggested to prepare these files in advance.</p>"},{"location":"input-full-data/#pgen-format","title":"pgen format","text":"<ul> <li>input format: <code>'pgen'</code></li> <li>input dataset: <code>'path/to/my_data'</code></li> </ul> <p>For pgen input, you have to specify only the basename of the dataset. Given a input dataset named <code>my_dataset</code>, the pipeline will look for the following files: <code>my_data.pgen</code>, <code>my_data.pvar</code>, <code>my_data.psam</code>.</p>"},{"location":"input-full-data/#bed-format","title":"bed format","text":"<ul> <li>input format: <code>'bed'</code></li> <li>input dataset: <code>'path/to/my_data'</code></li> </ul> <p>For bed input, you have to specify only the basename of the dataset. Given a input dataset named <code>my_dataset</code>, the pipeline will look for the following files: <code>my_data.bed</code>, <code>my_data.bim</code>, <code>my_data.fam</code>.</p>"},{"location":"input-full-data/#input-dataset-split-by-chromosome","title":"Input dataset split by chromosome","text":"<p>If your input dataset is split by chromosome across multiple files, you can use the <code>{CHROM}</code> tag in your input file name. This tag must be placed corresponding to the number of chromosome in the filename. When using this method be careful that the chromosome names captured from the filename correspond to numbers 1-22 for autosomes.</p> <p>For example, if you have a dataset split by chromosome in the following way:</p> <ul> <li><code>my_dataset_chr1.bgen</code></li> <li><code>my_dataset_chr2.bgen</code></li> <li><code>my_dataset_chr3.bgen</code></li> <li>...</li> </ul> <p>You can specify the input dataset as <code>my_dataset_chr{CHROM}.bgen</code> and the pipeline will automatically replace <code>{CHROM}</code> with the chromosome number.</p>"},{"location":"input-full-data/#performance-tips","title":"Performance tips","text":"<p>Note that the pipeline will do highly parallelized access to the input dataset. Thus, especially if you are using a single file as input and when step 2 chunk size is small, it is reccomended to use a high performance filesystem optimized for parallel access.</p> <p>Regenie step2 will run faster on bgen v1.2 with 8 bits encoding. You can convert existing data using plink2 with <code>--export bgen-1.2 'bits=8'</code> option</p>"},{"location":"input-indep-snps/","title":"QCed independent SNPs","text":"<p>To perform regenie step 1 you should provide a {bed,bim,fam} dataset containing independent SNPs with reasonable QC. The step 1 input dataset is defined by <code>genotypes_array</code> parameter. This need to be specified as prefix, so if you a dataset named <code>my_dataset.bed/bim/fam</code> you should specify <code>my_dataset</code> as input.</p> <p>Ideally, this would contain ~500k SNPs obtained after standard QC and pruning and MUST contain less than 1M SNPs. Usual pre-processing steps are:</p> <ul> <li>SNP missing rate &lt; 0.05</li> <li>sample missing rate &lt; 0.1</li> <li>MAF &gt; 0.01</li> <li>HWE p-value &gt; 1e-15</li> <li>variants pruning for LD (e.g. <code>--indep-pairwise 1000 100 0.8</code> in plink2)</li> </ul> <p>An additional QC will be automatically performed on this file since step1 requires strict filtering criteria. Additionally, pruning can also be performed by setting <code>prune_enabled = true</code>.</p>"},{"location":"input-ld-panel/","title":"LD panel (optional)","text":"<p>If you are analysing a large dataset with more than 50k samples, to speed up LD computation for clumping we suggest to prepare by chromosome bed/bim/fam files with the same variants present in the full genotype data but only a subset of samples. Then you can specify a pattern to these files using the <code>ld_panel</code> parameter like <code>/ld_panel/chr{CHROM}_ld</code>.</p> <p>The <code>{CHROM}</code> is automatically substituted with number 1-23 when the pipeline is running. This is only processed when the <code>clumping</code> option is active. If the <code>ld_panel</code> parameter is not set and clumping is active the pipeline will use the full genotype data to estimate LD. Note that this will result in very long run time for huge datasets so providing an LD panel is highly reccomended when sample size is above 50k.</p> <p>You can generate LD files using plink2 and the <code>--keep</code> option to extract a subset of unrelated samples from the full dataset. For example:</p> <pre><code>chrom=1 #Chromosome id\nsamples=\"samples.tsv\" #File with a subset of unrelated individuals\n\nplink2 \\\n--bgen input.bgen ref-first \\\n--sample input.sample \\\n--keep ${samples} \\\n--chr ${chrom} \\\n--make-bed \\\n--out chr${chrom}_ld_panel\n</code></pre>"},{"location":"input-ld-panel/#when-ld-panel-is-not-available","title":"When LD panel is not available","text":"<p>If you do not provide an LD panel, the pipeline will use the full input dataset to compute LD. First, input file(s) will be converted to bed format using plink2. Then if the input files are organized by chromosomes using the <code>{CHROM}</code> placeholder, only the relevant files will be used to perform clumping. Otherwise, if there are multiple input files, all converted dataset will be merged into a single dataset and this is used to perform clumping.</p> <p>In any case, clumping procedure is then performed parallelized by chromosome and resulting annotated loci are then merged into a single file per phenotype.</p>"},{"location":"input-models-table/","title":"Models table","text":"<p>When executing a multi-models run you need to provide all phenotypes and covariates in a single file using the <code>phenotypes_filename</code> parameter. The exact data types and column names for phenotypes and covariates are then configured in the models table.</p> <p>The parameter <code>models_table</code> is used to activate a multi-models run by configuring a models table. The models table is a tab-separated file with header and columns names: model_id, model, trait_type, genetic_model, cat_var.</p> <ul> <li>model_id: a unique identifier for the model.</li> <li>model: model descrition using col names from the phenotype table in the form <code>phenotype ~ covar1 + covar2 + ...</code>. Use <code>phenotype ~ 1</code> if you don't have any covariate</li> <li>trait_type: 'log' (for binary phenotype) or 'quant' (for quantitative phenotype).</li> <li>genetic_model: 'additive', 'dominant' or 'recessive'.</li> <li>cat_var: comma-separated list of categorical covariates. Use NA if none is present. Note that a max of 10 levels are accepted for a catagorical covariates by default, but this can be changed using the <code>maxCatLevels</code> parameter.</li> </ul> <p>An example of a models table is shown below:</p> <pre><code>model_id        model   trait_type      genetic_model cat_var\nM1      QP1 ~ Q1+Q2+Q5  quant   additive  NA\nM2      QP2 ~ Q1+Q2+Q5  quant   additive  Q1,Q2\nM3      QP3 ~ Q1+Q2+Q5  quant   additive  NA\nM4      QP4 ~ 1  quant   additive   NA\n</code></pre>"},{"location":"input-phenotype-file/","title":"Phenotype and covariates","text":"<p>To perform the analysis you should provide one tab-separated file containing the phenotypes(s) of interest and optionally and additional tab-separated files containing the covariates to be included in the analysis. In the end, only samples found in both phenotype and covariates tables will be processed.</p>"},{"location":"input-phenotype-file/#phenotypes","title":"Phenotypes","text":"<ul> <li>The phenotype input table is defined with the <code>phenotypes_filename</code> parameter and must be a tab-separated file with header defining the phenotype(s). The first 2 columns must be FID, IID.</li> <li>The exact list of column names to process are then defined with the <code>phenotypes_columns</code> parameters. </li> <li>Set <code>phenotypes_binary_trait</code> to true or false according to the type of phenotype(s). Note that quantitative and binary traits can not be mixed in single mode run.</li> </ul>"},{"location":"input-phenotype-file/#notes-for-multi-models-run","title":"Notes for multi-models run","text":"<ul> <li>In multi-models mode, all phenotypes and covariates values must be provided in a single tab-separated file with header and configured as <code>phenotypes_filename</code>. The exact data types and column names for phenotypes and covariates are then configured in the models table.</li> <li>When using multi-model mode, the phenotype file can have a single <code>IID</code> column as first column. In this case, the <code>FID</code> will be deduced from the fam file in the <code>genotypes_array</code> dataset.</li> </ul>"},{"location":"input-phenotype-file/#covariates","title":"Covariates","text":"<ul> <li>The covariates table is defined with the <code>covariates_filename</code> parameter and must be a tab-separated file with header defining the covariate(s). You can set this to 'NO_COVAR_FILE' when no covariates are present in your model.</li> <li>The exact list of column names to process are then defined with the <code>covariates_columns</code>. Here it is fine to mix binary and quantitative covars. </li> <li>Binary covars must contain only 0/1 values (otherwise they will be treated as numeric values).</li> <li>Columns containing categorical values must be specified with the <code>covariates_cat_columns</code> parameter. These will be automatically converted to dummy variables. The maximum number of categories allowed is 10 by default, but can be adjusted with the <code>maxCatLevels</code> parameter.</li> </ul> <p>Note that no missing values are allowed in this table and the pipeline will stop with an error in any missing value is found. This is made to avoid any ambiguity in the interpretation of the results. Indeed, regenie removes any individual with missing values in any covariate from the analysis thus we enforce providing covariates with no missing values to ensure control on the final sample size of the analysis.</p> <p>In the end, the actual samples used in the analysis will be the intersection of the samples found in the phenotype and covariates tables.</p>"},{"location":"input-phenotype-file/#processing-of-missing-phenotype-values","title":"Processing of missing phenotype values","text":"<p>Following the regenie documentation, keep in mind how missing phenotypes values are handled:</p> <ul> <li>Samples listed in pheno file that are not in bgen/bed/pgen file are ignored. Genotyped samples that are not in the pheno file are removed from the analysis.</li> <li>With quantitative traits, missing values are mean-imputed in Step 1 and they are dropped when testing each phenotype in Step 2</li> <li>With binary traits, missing values are mean-imputed in Step 1 when fitting the level 0 linear ridge regression and they are dropped when fitting the level 1 logistic ridge regression for each trait. In Step 2, missing values are dropped when testing each trait.</li> </ul>"},{"location":"input-phenotype-file/#note-when-using-vcf-files-as-input","title":"Note when using VCF files as input","text":"<p>When you use VCF files as input, these are internally converted to PGEN format. By default, the resulting dataset will have constant FID set to '0' and IID corresponding to the sample IDs in the VCF. You need to be sure that this nomenclature is consistent with the covariates and phenotype input files.</p>"},{"location":"input-projects-table/","title":"Projects table","text":"<p>When executing a multi-projects run you need to configure the genetic input data in the main configuration file, the other project-specific parameters will be taken from the projects table.</p> <p>The parameter <code>projects_table</code> is used to activate a multi-projects run by configuring a projects table. The projects table is a tab-separated file with header and the following columns (names are mandatory). You can use NA in the optional columns when the value is not relevant.</p> <ul> <li><code>project_id</code>: a unique identifier for the project</li> <li><code>pheno_file</code>: path to the phenotype file</li> <li><code>pheno_cols</code>: comma-separated list of phenotype columns</li> <li><code>pheno_binary</code>: True if the phenotype is binary, False otherwise</li> <li><code>pheno_model</code>: association model for this phenotype, can be additive, dominant or recessive</li> <li><code>cov_file</code>: path to the covariates file, use NO_COV_FILE if no covariates are used</li> <li><code>cov_cols</code>: comma-separated list of covariates columns (excluding categorical covariates), can be omitted if cov_file is NO_COV_FILE</li> <li><code>cov_cat_cols</code> (optional): comma-separated list of categorical covariates columns</li> <li><code>interaction_snp</code> (optional): a single variant ID to perform interaction analysis (GxG)</li> <li><code>interaction_cov</code> (optional): a single covariate name to perform interaction analysis (GxE)</li> <li><code>condition_list</code> (optional): a text file containing a list of variants IDs to condition on</li> <li><code>extract_snps_list</code> (optional): a text file containing a list of variants IDs to restrict GWAS analysis</li> <li><code>extract_genes_list</code> (optional): a text file containing a list of gene IDs to restrict rare variant analysis</li> </ul>"},{"location":"input-projects-table/#important-note-when-using-conditional-or-interaction-snp-analysis","title":"Important note when using conditional or interaction SNP analysis","text":"<p>When you specify <code>condition_list</code> or <code>interaction_snp</code> for one of your project, you must also configure the additional genotype datasets for the conditional analysis. This dataset should contain all the SNPs listed in the condition_list and interaction_snp and it's used to ensure that genetic information for these SNPs are available to all chunks. The same additional dataset is used across all projects at the moment.</p> <p>You have to set the following parameters:    </p> <ul> <li>additional_geno_file: prefix of the genotype dataset containing vars in condition_list or interaction snp. This is mandatory for conditional or interaction analysis</li> <li>additional_geno_format: can be bgen, pgen or bed.</li> </ul>"},{"location":"input-projects-table/#examples","title":"Examples","text":"<p>A minimal projects table can be as follows:</p> project_id pheno_file pheno_cols pheno_binary pheno_model cov_file cov_cols project1 phenos.txt qpheno1,qpheno2 False additive covars.txt cov1,cov2 project2 phenos.txt bpheno1,bpheno2 True additive NO_COV_FILE NA <p>An example project table with all columns can be as follows:</p> project_id pheno_file pheno_cols pheno_binary pheno_model cov_file cov_cols cov_cat_cols interaction_snp interaction_cov condition_list extract_snps_list project1 phenos.txt qpheno1,qpheno2 False additive covars.txt cov1,cov2 cat_covar1 NA NA NA project2 phenos.txt qpheno1,qpheno2 False additive covars.txt cov1,cov2 cat_covar1 rs12345 NA conditional_snps.txt project3 phenos.txt bpheno3,bpheno4 True additive NO_COV_FILE NA NA NA NA NA NA"},{"location":"input-rare-variants/","title":"Variant annotation files","text":"<p>When running a rare variant analysis additional variant annotation files are needed and must be provided using the following parameters:</p> <ul> <li> <p><code>rarevar_set_list_file</code>: a set list file linking variants to genes. Essentially a tab- or space-separated file with 4 columns: the set/gene name, the chromosome and physical position for the set/gene, a comma-separated list of variants included in the set/gene. This file is used to define the variant sets to be tested. The chromosome names must be numbers 1-22 for autosomes.</p> </li> <li> <p><code>rarevar_anno_file</code>: variant annotation file. A tab- or space-separated file with 3 columns: variant name, the set/gene name, a single annotation category (for example missense, LoF, ...). Variants not in this file will be assigned to a default \"NULL\" category. A maximum of 63 annotation categories (+NULL category) is allowed. </p> </li> <li> <p><code>rarevar_mask_file</code>: mask definition file. A tab- or space-separated file with 2 columns: a mask name followed by a comma-separated list of variant categories included in the mask.</p> </li> </ul> <p>More information on the exact foramt of these files is available in the regenie documentation.</p> <p>For each gene defined in the set list file, regenie will apply the configured tests separately for each mask defined in the mask file (thus including only the variant categories described by each mask).</p>"},{"location":"input-reuse-step1/","title":"Re-use step 1 predictions","text":"<p>If you set <code>save_step1_predictions</code> to true, the pipeline will save the step 1 predictions in <code>regenie_step1_preds</code> folder. </p> <p>These predictions can be re-used in  further analyses on the same dataset when phenotype(s) and covariates are the same. This is useful for example to perform a rare variant analysis on the same dataset used for a previous GWAS analysis.</p> <p>You can load level 1 preds from this folder in subsequent analyses by setting <code>regenie_premade_predictions</code> to a path like <code>/results/regenie_step1_preds/regenie_step1_out*</code>.</p> <p>The pipeline then expects the following:</p> <ul> <li>A file named <code>regenie_step1_out_pred.list</code> must be present</li> <li>One file per phenotype is expected named <code>regenie_step1_out_1.loco.gz</code> <code>regenie_step1_out_2.loco.gz</code>, ...</li> <li>phenotypes and covariates used in the new analyses must be exactly the same used to generate step1 predictions and column names for phenotypes must be configured in <code>phenotypes_columns</code> in the same order as in the original analysis.</li> </ul>"},{"location":"main-parameters/","title":"Main parameters","text":"<p>A template for preparing a configuration file for a new project is provided in <code>templates/single_project.conf</code>. You can copy this file and edit it to adjust the parameters for your analysis.</p> <p>Here is a list of the main parameters you need to adjust for a new analysis:</p> <ul> <li> <p>Set <code>project</code> to a custom string defining your project ID. Note that max 50 chars are allowed and no special characters. This will be used in reports and a folder will be created with the same name to store all results.</p> </li> <li> <p>Set <code>chromosomes</code> to represent the list of chromosome to be included in the analysis. You can use a comma-separated list of chromosome numbers, a range like <code>1-22</code> or a mix like <code>1,4,11-18</code>.</p> </li> <li> <p>You can eventually restrict the analysis to a specific genomic range, or a specific list of SNP IDs or gene IDs using the <code>regenie_range</code>, <code>regenie_extract_snps</code> and <code>regenie_extract_genes</code> options. This requires <code>step2_gwas_split</code> and <code>step2_rarevar_split</code> to be set to <code>false</code>. Keep in mind that in this case you must ensure that the provided SNPs/genes/region are present in the genotype dataset you provided as input and in the specified chromosomes. Otherwise, the pipeline will fail due to SNPs remaining for step2 analysis.</p> </li> <li> <p>Set <code>genotypes_build</code> to the build of your genotype data, either hg19 or hg38.</p> </li> <li> <p>Set <code>step2_gwas_chunk_size</code> and <code>step2_rarevar_chunk_size</code> according to the size of your dataset. These control how the dataset is split for step2 analysis. Teh default values usually work fine, but you can increase/decrease them if you have very large or small datasets. Keep in mind that a small chunk size may result in a very large amount of parallel jobs. By default the pipeline job submission rate is limited to 200 concurrent jobs. The total number of jobs will be N_SNPs/gwas_chunk_size for GWAS and N_genes/rare_chunk_size for rare variant analysis.</p> </li> <li> <p>For GWAS analysis, set <code>regenie_gwas_test</code> to the type of model you want to test, either 'additive', 'dominant' or 'recessive'.</p> </li> <li> <p>For rare variant test, set <code>rarevars_vc_test</code> as a comma-separated list of rare variants tests to perform among those accepted by regenie: skat, skato, skato-acat, acatv, acato, acato-full. You can also set <code>rarevars_joint_test</code> to perform a joint test across burden masks / bins. Possible values are: minp, acat, sbat.</p> </li> <li> <p>Set <code>regenie_gwas_min_mac</code> and <code>regenie_rarevar_min_mac</code> to control the min allowed MAC for variants tested in either GWAS or rare variant analysis. Variants with MAC below this threshold will be excluded from the analysis.</p> </li> <li> <p>Set <code>annotation_min_log10p</code> to the min value ( -log10(pval) ) for top hit SNPs from GWAS results. These SNPs are also annotated in the manhattan plot in the HTML report.</p> </li> <li> <p>Set <code>rarevar_min_log10p</code> to the min value ( -log10(pval) ) for top hit genes from rare variants analysis. These genes are also annotated in the general manhattan plot in the HTML report.</p> </li> <li> <p>Set <code>clump_p1</code> to the maximum pvalue allowed for index SNPs during plink clumping to define top loci</p> </li> <li> <p>If you are analyzing many phenotypes on a large dataset, we suggest to set <code>make_report</code> to false or at least disable the generation of the locus zoom plots by setting <code>n_top_loci_plot</code> to zero. Generating the HTML graphical report and especially the regional plots, can add a considerable amount of time when many phenotypes are tested together on a large dataset.</p> </li> <li> <p>If you have categorical covariates, the maximum number of allowed categories is set to 10 bu default. You can adjsut this using the <code>maxCatLevels</code> parameter.</p> </li> </ul>"},{"location":"main-parameters/#input-files","title":"Input files","text":"<p>Set the input files parameters as described in the input files sections.</p> <ul> <li>The full genetic dataset to perform GWAS analysis</li> <li>The reduced SNPs dataset to perform REGENIE step1 regression</li> <li>The rare variants dataset and annotations to perform rare variant tests</li> <li>An optional set of files representing your LD panel to perform loci clumping</li> </ul>"},{"location":"main-parameters/#multi-models-run","title":"Multi-models run","text":"<p>In case you want to configure a you also need to set the following parameters:</p> <ul> <li> <p>Set <code>models_table</code> to a tab-separated file defining the models to test</p> </li> <li> <p>Set <code>missing_tolerance</code> to the maximum allowed fraction of missing phenotype values when collecting uniform group of phenotypes for a run.</p> </li> </ul>"},{"location":"main-parameters/#multi-projects-run","title":"Multi-projects run","text":"<p>In case you want to configure a multi-projects run you also need to set the following parameters:</p> <ul> <li>Set <code>projects_table</code> to a tab-separated file defining the projects to test</li> </ul>"},{"location":"main-parameters/#conditional-and-interaction-analysis","title":"Conditional and interaction analysis","text":"<p>In case you want to perform conditional or interaction analysis, you can configure the following parameters:</p> <ul> <li><code>interaction_cov</code>: run GxE test specifying the interacting covariate from covariate table</li> <li><code>interaction_snp</code>: run GxG test specifying the interacting variant ID</li> <li><code>condition_list</code>: run conditional analysis specifying a file with variant IDs to condition on</li> </ul> <p>Note that to perform conditional/interaction analysis, an additional genotype dataset must be provided containing the SNP(s) used for conditioning/interaction. This can be configured using the following parameters:</p> <ul> <li><code>additional_geno_file</code>: prefix of the genotype dataset containing vars in condition_list or interaction var. This is mandatory for conditional or interaction analysis</li> <li><code>additional_geno_format</code>: can be bgen, pgen or bed.</li> </ul> <p>See the conditional analysis section for more details.</p>"},{"location":"outputs/","title":"Outputs","text":"<p>By default, the pipeline will generate all results in a folder named according to the <code>project</code> parameter: <code>&lt;project&gt;_output</code>. Main results of variant and gene based analyses are then stored in this folder using the <code>project</code> parameter (i.e. <code>&lt;project&gt;_output/&lt;project&gt;</code>). When you provide the <code>outdir</code> parameter, this will be used as the main output folder, thus the association results will be placed in <code>&lt;outdir&gt;/&lt;project&gt;</code>.</p> <p>When you are running in multi-model or multi-project mode you will end up having multiple sub-folders, one for each project id.</p> <p>Main structure of the output folder is as follows:</p> <pre><code>.\n|-- analysis_config\n|-- step2_chunks\n|   |-- step2_dataset_autosomes.GWAS-chunks.txt\n|   `-- step2_dataset_autosomes.genes-chunks.txt\n`-- project1\n    |-- logs\n    |-- regenie_step1_preds\n    |-- reports\n    |-- results\n    |   |-- gwas\n    |   `-- rare_var\n    `-- validated_input\n</code></pre> <ul> <li>validated input tables for phenotyopes and covariates are saved in <code>validated_input</code> folder. These are the actual tables used in the analysis after validation and formatting (sanitize column names, remove spaces, etc.).</li> <li>main results from GWAS and rare variants tests are in <code>results</code>. For GWAS, top associated SNPs annotated for gene(s) overlap and nearby gene(s) are saved in the <code>tophits</code> sub-folder, and top loci after clumping are saved in <code>toploci</code> sub folder. For rare variants, top associated genes are saved in <code>tophits</code> sub folder.</li> <li>step 1 predictions in <code>regenie_step1_preds</code> can be reused for further analyses on the same dataset as long as the input bgen, phenotype file and covars file are exactly the same and phenotypes and covars list are provided in the same order.</li> <li>general configuration files for the pipeline execution are saved in <code>analysis_config</code> folder.</li> <li>logs from all project-specific operations are saved in <code>logs</code> for debugging.</li> <li>HTML reports for GWAS and rare variants are saved in <code>reports</code> folder when <code>make_reports</code> is true.</li> </ul>"},{"location":"parallelization/","title":"Parallelization","text":"<p>The pipeline optimize the run time by splitting heavy tasks into small chunks. The default settings are optimized for biobank scale data, but you can adjust resources as needed to further cut down run time or to process smaller datasets. You can change the default minimum resources used at each step by editing the file <code>conf/base.config</code>, or providing a new config file with the <code>-c</code> option when you run the pipeline.</p> <p>First of all, keep in mind that the analysis of binary phenotypes requires much more resources and take longer time. In general we suggest to analyze not more than 100 quantitative phenotypes or 50 binary phenotypes in a single run. If you need to analyze more phenotypes, you can take advantage of the multi-model run mode to split the analysis in multiple runs.</p> <p>You can optimize the pipeline scaling by adjusting the following parameters:</p> <ul> <li><code>step1_n_chunks</code>: this controls the number of chunks for step1 L0 regression. The default of 100 is usually fine in most case and can eventually be reduced for small datasets with less than 10 thousands samples.</li> <li>step1 L1 is then performed by phenotype. This behaviour can not be changed. If you are analyzing a large number of binary phenotypes (more than 50 in a single run) on biobank scale date (more than 200k individuals) you will probably need to increase the min memory for <code>step1_runl1</code> label to 16 or 24G.</li> <li><code>step2_gwas_chunk_size</code>: this controls the number of variants analyzed in each chunk during regenie step2. Reducing this number increase the number of concurrent chunks and reduce run time per chunk, thus increasing overall performance when enough resources are available. Default is 50000. This can be increased to 100000 or even 250000 for small datasets with less than 100k individuals.</li> <li><code>step2_rarevar_chunk_size</code>: this controls the number of genes from the input snpset file analyzed in each chunk during regenie step2 for rare variant analysis. Reducing this number increase the number of concurrent chunks and reduce run time when enough resources are available. Default is 100.</li> </ul> <p>Another important aspect to consider is the maximum number of jobs submitted simultaneously. This can be asjuted using the <code>queueSize</code> parameter in the executor scope when you create your custom profile. If you have a large cluster with many resources available, you can set this to a large value to submit all jobs simultaneously. This will reduce the overall run time when enough resources are available.</p> <p>To further increase speed, you can eventually adjust the default amount of resources requested for regenie step1 and regenie step2 operations by editing the file <code>conf/base.config</code>. The default here are sensible for most cases, but you can increase the amount of memory and CPUs requested for each task to reduce computation time. In particular you can act on the following labels in the configuration file to adjust the resources used as starting point in step1 and step2: step1_runl0, step1_runl1, step2_gwas, step2_rarevar. Resources allocation is dynamic, so when a process fail due to OOM or OOT it will be retried automatically increasing the resources.</p> <p>In our tests using the default settings one can analyze 100 quantitative phenotypes on the full UKBB dataset (about 500k individuals and 45M SNPs) including loci identification, but withouth HTML reports, in ~7h under normal HPC load considering a limit of 200 concurrent tasks (peak of 800 CPUs usage).</p>"},{"location":"quick-start/","title":"Quick Start","text":"<ol> <li> <p>Create a folder for your project (e.g. <code>yourproject</code>)</p> </li> <li> <p>Prepare a tab-separated table of phenotypes and eventually covariates (see the input section).</p> </li> <li> <p>Prepare and configure the required input data for step 2, usually an imputed or sequencing dataset, and step 1, usually a QCed and pruned dataset. You can eventually prepare also a set of files for LD computation, suggested if you want to run loci clumping when analyzing a large dataset with &gt; 100k samples.</p> </li> <li> <p>If you want to perform a multi-models or multi-projects execution, prepare the models table or projects table to describe your analyses.</p> </li> <li> <p>If you want to perform conditional or interaction analysis, prepare the additional genotype dataset and eventually conditional variant list as described in the conditional analysis section.</p> </li> <li> <p>Prepare the necessary config files, using the templates provided in the <code>templates</code> folder:</p> </li> <li>A config file describing settings and inputs for your project.</li> <li>A config file to define the profile for your computational environment.</li> <li> <p>Optionally, you can also add configuration to enable execution monitoring using Nextflow Tower</p> </li> <li> <p>Invoke the pipeline using <code>nextflow run HTGenomeAnalysisUnit/nf-pipeline-regenie</code></p> </li> </ol> <p>Usually, you want to prepare a script to submit the pipeline in your project folder. In this example we use <code>sbatch</code> submission system, but this can be adapted to any scheduler. <code>myprofile</code> corresponds to a profile you created for your computational environment:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name nf-regenie\n#SBATCH --output nf-regenie_master_%A.log\n#SBATCH --partition cpuq\n#SBATCH --cpus-per-task 1\n#SBATCH --mem 8G\n#SBATCH --time 1-00:00:00\n\nmodule load nextflow/22.10.1 singularity/3.8.5\n\nexport NXF_OPTS=\"-Xms1G -Xmx8G\" \nnextflow run HTGenomeAnalysisUnit/nf-pipeline-regenie \\\n   -profile singularity,myprofile -c your_project.conf\n</code></pre> <p>Alternatively, you can clone the latest pipeline version using</p> <p><code>git clone --depth 1 https://github.com/HTGenomeAnalysisUnit/nf-pipeline-regenie.git</code></p> <p>This will create a new folder called <code>nf-pipeline-regenie</code> in the current folder containing all the pipeline files.</p> <p>You can eventually chose a specific version of the pipeline using the <code>--branch</code> option</p> <p><code>git clone --depth 1 --branch v1.8 https://github.com/HTGenomeAnalysisUnit/nf-pipeline-regenie.git</code></p>"},{"location":"running-modes/","title":"Execution modes","text":""},{"location":"running-modes/#single-project","title":"Single project","text":"<p>The single model execution mode represent a standard regenie assoiation analysis. Given a set of phenotypes variables all of the same type (quantitative or binary) and a set of covariates you can perform both association test at single variant level (GWAS) or rare variants aggregated tests.</p> <p>Ideally, none of the tested phenotype should have a large number of missing values (10-15% is a reasonable threshold) since missing values are automatically imputed by regenie in some steps and this may introduce biases. Also keep in mind that samples with missing values in any of the covariates will be excluded from the analysis.</p> <p>You can test multiple phenotypes together in the same execution as long as they are of the same type, namely all binary or quantitativa variables, and they are all tested with the same set of covariates. Regenie is able to manage large phenotypes batches in a single execution. When working with biobank scale datasets (&gt;200k individuals and &gt;20M variants) our suggestion is to limit each analyses to 100 quantitative phenotypes or 50 binary phenotypes, but you can run larger batches if you have enough computational resources or a smaller datasets.</p> <p>In single project mode, all parameters for the analysis are taken from the configuration file. See the main parameters section for more details on how to prepare the configuration file.</p>"},{"location":"running-modes/#multi-models","title":"Multi-models","text":"<p>When using multi-models execution mode one can test any number of arbitrary models (combinations of pheotypes and covariates) on a given cohort. In this case, user needs to provide teh following parameters:</p> <ul> <li><code>models_table</code>: a models tables describing the models of interest</li> <li><code>phenotypes_filename</code>: a tab-separated file containing all the traits (pheotypes and covarites)</li> <li><code>missing_tolerance</code>: a threshold for the maximum fraction of missing values in phenotype variables</li> <li><code>pheno_chunk_size</code>: a threshold for the maximum size of a phenotype batch</li> </ul> <p>Based on this information we first perform a preparing step that configure uniform analysis runs by grouping together compatible models. The process works as follows:</p> <ol> <li>Models with the same phenotyped data type, the same genetic model and the same set of covariates are grouped together</li> <li>For each model a matrix is generated containing the phenotype variables and all the covariates used in the model. Individuals with any missing value in any covariate are then excluded.</li> <li>The resulting model matrixes are then grouped to create multi-phenotype analysis in which none of the tested phenotype has more then the allowed fraction of missing values and no missing values are present in any covariates</li> <li>Analysis chunks are then prepared based on the configured phenotype batch size</li> <li>The resulting analysis chunks are automatically submitted to the pipeline for association analysis</li> </ol> <p>NB. The pipeline removes individuals with any missing value in covariates, to be consistent with the REGENIE behaviour. As a result of this process the final sample size available for the analysis may be largely reduced if you use covariates with a large fraction of missing values. In this case we suggeste to pre-process covariate data to impute missing values or remove the covariates with a large fraction of missing values.</p> <p>A main result folder is created under <code>outdir</code> (or using the default nf-regenie-pipeline_results) where models configuration is saved. Then results for each analysis chunk are saved in a separate sub-folder containing the configuration and the exact tables of phenotypes and covariates used in the execution.</p> <p>In this way multiple hypotheses can be easily tested on large cohort with minimum amount of data pre-processing.</p> <p>Note that in the output all phenotypes are renamed according to the model id defined in the input models table. In this way, each phenotype code in the output represents a unique model (combination of a phenotype and covariates). So for example, suppose you configured a multi-model run to test the phenotype <code>height</code> with 2 different sets of covariates:</p> model_id model M1 height ~ cov1+cov2 M2 height ~ cov3+cov4 <p>The resulting output traits will be named <code>M1</code> and <code>M2</code> and the output files containing summary statistics will be named <code>M1.regenie.gz</code>, <code>M2.regenie.gz</code>.</p> <p>See the multi models configuration section for more details on how to prepare the input files for a multi-models run.</p>"},{"location":"running-modes/#multi-projects","title":"Multi-projects","text":"<p>Multi-projects mode can be used to conveniently perform multiple project runs on the same genetic data in a single execution. In this case, the user needs to provide the <code>projects_table</code> parameter to specify a tab-separated file describing the projects to run. See the projects table section for more details on how to prepare this file.</p> <p>Values from this table will be used to configure multiple analyses and will over-ride any value provided in the config files.</p> <p>A main result folder is created under <code>outdir</code> (or using the default nf-regenie-pipeline_results) where models configuration is saved. Then results for each analysis chunk are saved in a separate sub-folder containing the configuration and the exact tables of phenotypes and covariates used in the execution.</p>"},{"location":"top-hits-annotation/","title":"top hits annotation","text":""},{"location":"top-hits-annotation/#annotation-of-tophits","title":"Annotation of tophits","text":"<p>For top hits SNPs / loci the pipeline performs automatic annotation with overlapping genes and nearby genes. </p> <p>The maximum distance between the variant / locus and the sorrounding annotated gene is set with the parameter <code>annotation_interval_kb</code> (25kb by default). Genes located +/- defined kb from the SNP / locus are annotated as nearby genes.</p> <p>Pre-made BED files for gene annotation are available for GRCh37 and GRCh38. These files are generated from the GENCODE annotation version 39. When performing annotation one can chose to annotate for all genes or only protein coding genes by setting the <code>genes_group</code> parameter to <code>all</code> or <code>protein_coding</code> respectively. By default, the pipeline will annotate for protein coding genes.</p> <p>If you want to use a different annotation file, you can provide custom gene definition with the <code>genes_bed</code> and <code>genes_ranges</code> parameters. In this case the pre-made annotation files will not be used. For this you need to provide:</p> <ul> <li><code>genes_bed</code>: a regular tab-separated BED file (chromosome, start, end; zero-based), with gene names in the 4th column. Note that a name must be present for all genes.</li> <li><code>genes_ranges</code>: essentially the same as above but space separated.</li> </ul>"},{"location":"tower-monitoring/","title":"Monitoring with Tower","text":"<p>To easily monitor pipeline execution, we suggest to use Nextflow Tower. </p> <ol> <li>First, register to the Nextflow tower using your GitHub or Google account. </li> <li>Then, click on your profile (upper right corner) and select <code>Your tokens</code>. Then click add token and follow the instructions to create a new token. Make sure to copy the generated token, since you were not able to see it again.</li> <li>Add the following to <code>single_project.conf</code> file or create a separate <code>tower.conf</code> file and then run the pipeline adding <code>-c tower.conf</code>:</li> </ol> <pre><code>tower {\n  enabled = true\n  accessToken = 'your_token_here'\n}\n</code></pre> <p>Now when the pipeline is running you should be able to monitor progress in the Nextflow tower under your runs.</p>"}]}